---
################################
# Experiment Parameters        #
################################
experiment:
  # Optimisation parameters
  use_saved_learning_rate: false
  learning_rate: 1.0e-03
  lr_exp_decay: true
  lr_exp_decay_offset: 200  # epochs
  lr_exp_decay_final_learning_rate: 1.0e-05  # instead of using decay rate, set the final learning rate
  weight_decay: 1.0e-06
  grad_clip_thresh: 1.0
  batch_size: 32
  mask_padding: true  # set model's padded outputs to padded values
  gate_loss_pos_weight: 8.0

  # Epochs
  epochs: 2000

  # Misc
  iters_per_checkpoint: 1000
  seed: 1234
  dynamic_loss_scaling: false # true in nvidia git
  fp16_run: false
  distributed_run: false
  dist_backend: "nccl"
  dist_url: "tcp://localhost:54321"
  cudnn_enabled: true
  cudnn_benchmark: false
  ignore_layers: [ 'embedding.weight' ]


################################
# Data Parameters             #
################################
data:
  # File locations
  load_mel_from_disk: false
  num_speakers: 4  # TODO: replace with the relevant information
  training_files: "/path/to/dataset/train_filelist.txt"
  validation_files: "/path/to/dataset/val_filelist.txt"
  text_cleaners: ['lv_cleaners']

  # Audio parameters
  max_wav_value: 32768.0
  sampling_rate: 24000
  filter_length: 1024
  hop_length: 256
  win_length: 1024
  n_mel_channels: 80
  mel_fmin: 0.0
  mel_fmax: 8000.0


################################
# Model Parameters             #
################################
model:
  n_symbols: "dummy"   # TODO: replace with the relevant information
  symbols_embedding_dim: 512

  # Encoder parameters
  encoder_kernel_size: 5
  encoder_n_convolutions: 3
  encoder_embedding_dim: 512

  # Decoder parameters
  n_frames_per_step: 1  # currently only 1 is supported
  decoder_rnn_dim: 1024
  prenet_dim: 256
  max_decoder_steps: 1000
  gate_threshold: 0.5
  p_attention_dropout: 0.1
  p_decoder_dropout: 0.1

  # Attention parameters
  attention_rnn_dim: 1024
  attention_dim: 128

  # Location Layer parameters
  attention_location_n_filters: 32  # number of location attention filters.
  attention_location_kernel_size: 31  # filter size of location attention convolution layer.

  # Mel-post processing network parameters
  postnet_embedding_dim: 512
  postnet_kernel_size: 5
  postnet_n_convolutions: 5

################################
# Multi-speaker and GST        #
################################
use_speaker_embedding: true  # use speaker embedding to enable multi-speaker learning.
use_gst: true  # use global style tokens
speaker_embedding_dim: 512
use_external_speaker_embedding_file: false  # if true, forces the model to use external embedding per sample instead of nn.embeddings, that is, it supports external embeddings such as those used at: https://arxiv.org/abs /1806.04558
external_speaker_embedding_file: "../../speakers-vctk-en.json"
gst:
  gst_embedding_dim: 256
  gst_num_heads: 8
  gst_style_tokens: 10
  gst_use_speaker_embedding: false
